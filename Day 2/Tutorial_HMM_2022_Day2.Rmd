---
title: "Hidden Markov Models: Missing data and multiple data streams"
editor_options:
  chunk_output_type: console
output:
  html_document:
    number_sections: true
  pdf_document: 
    number_sections: true
---

# Tutorial goals and set up

## objectives

-   Dealing irregular locations,

-   time gaps,

-   including diving covariates

-   narwhal data

# import data

First, we'll setup the workspace with required packages

```{r message=FALSE}
library(momentuHMM)
library(dplyr)
library(tidyr)
library(lubridate)
library(adehabitatLT)
library(sf)
library(tmap)
library(units)
library(stringr)
require("conicfit")
```

For simplicity, we will also only look at the data for the month of August, 2017.
```{r message=FALSE}
tracks <- read.csv("data/tracks.csv") %>%
  filter(!is.na(x) & !is.na(y)) %>% # remove missing locations
  mutate(
    time = ymd_hms(time), # define time
    loc_class = factor(loc_class, # define location class factor levels
      levels = c("GPS", 3, 2, 1, 0, "A", "B"))) %>% 
  # remove identical records
  filter(!(time == lag(time) & x == lag(x) & y == lag(y) & loc_class == lag(loc_class)),
  # filter only the month of august
  month(time) == 8, day(time) > 7,  day(time) <= 14)

```

The classic HMM assumes the observation data is in discrete time and that there is no missing data in the predictor variables

```{r}
# calculate time difference between locations
tracks <- tracks %>%
  mutate(dt = ifelse(ID == lead(ID), # If next data row is same individual
    difftime(lead(time), time, units = "mins"), NA
  )) # calculate time difference

hist(tracks$dt, 1000, main = NA, xlab = "Time difference (min)")
```

Next, we'll convert the data to a spatial dataset using the `sf` package and plot the data.
First, we define the coordinate reference system of the original data (in this case WGS84, which is defined by the EPSG code `4326`). Next, we will project the data into NAD83(CSRS) UTM zone 21N (EPSG:2962), which will projected the coordinates in meter units with minimal distortion for this data set. 

```{r define projection, message=FALSE}
tracks <- tracks %>%
  st_as_sf(coords = c("x", "y")) %>% # converts to an sf object
  st_set_crs(4326) %>% # define CRS
  st_transform(2962) # reproject data to a UTM
```

Now, we can map the data using the `tmap` package to visualise what it looks like.
```{r plot original data, message=FALSE}
# convert to lines and plot using tmap
tracks %>% group_by(ID) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING") %>% 
     tm_shape()+
   tm_lines(col = "ID", palette = "Dark2")
```

It looks like there are at least several outlier points. We can address most of these using a coarse speed filter. However, before we can calculate speed ($\frac{\Delta dist}{\Delta t}$), we must address the duplicate time-stamps (since we cannot divide by 0 to calculate speed). It is often not possible to reliably identify which record is more accurate to discard. Therefore, it's best to retain as as much data and offset duplicate times slightly. We will first offset duplicate times by adding 10s for each consecutive row with a `dt == 0`.

```{r message=FALSE}
# first, we count the run length of each time step using rle
run_length <- rle(tracks$dt)$lengths
# second, we create a sequence of 1:run_length for each time step
run_seqence <- sequence(run_length)
# third, we discount each time step that's not a duplicate by multiplying by 0
run_seqence <- run_seqence*(tracks$dt==0)
# last, we lag run_sequence by one row and replace the first value by 0,
# so that we itterate each time that is a duplicate of the previous row
tracks$duplicate_count <- replace_na(lag(run_seqence), 0)
# offset duplicate times by 10s
tracks <- 
  tracks %>%
  mutate(time = time + 10*duplicate_count,
         # recalculate time difference
         dt = ifelse(ID == lead(ID), 
    difftime(lead(time), time, units = "mins"), NA
  ))

# last we ensure there are no identical times
sum(tracks$time == lead(tracks$time), na.rm = T)
```

Now, we can apply a speed filter to remove locations that are faster than some threshold -- in this case we will exclude any locations $>10$ m/s from the previous location.

```{r speed filter, message=FALSE}
# distance from previous location 
delta_dist <- st_distance(tracks, lag(tracks), by_element=TRUE)
# time from previous location
delta_t <- set_units(lag(tracks$dt), "min") 
# calculate speed
tracks$spd <- delta_dist/delta_t

# filter speed
tracks <- tracks %>% 
  filter(spd < set_units(10, "m/s"))

# convert to lines and plot
tracks %>% group_by(ID) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING") %>% 
     tm_shape()+
   tm_lines(col = "ID", palette = "Dark2")
```
Already looking much better.

For the first part of this tutorial, we'll use only the fastloc GPS data so we don't have to deal with location error. 

```{r filter and plot gps data, message=FALSE}
# filter GPS locations only
tracks_gps <- tracks %>% 
  filter(loc_class=="GPS")

# plot GPS
tracks_gps %>% 
  group_by(ID) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING") %>% 
     tm_shape() +
   tm_lines(col = "ID", palette = "Dark2")
```
We lose some data, particularly near the end of the tracks, but we will integrate ARGOS locations later in this tutorial.

## selecting a time interval for the HMH
there are two key decisions we must make, the temporal resolution to use, and how to address data gaps. The desired resolution depends predominantly on the biological question you are asking as different behaviours and biological processes occur at different spatial and temporal scales (e.g. seasonal migration, daily movement between foraging and resting grounds, and fine scale foraging decisions). Generally, higher resolution data is preferred as it has more information, however it is possible to have too-high of a resolution wherein information from fine-scale variability drowns out the signal from coarse-scale patterns of interest (e.g., seasonal migration). In this case, we will be linking the movement data with high resolution (75 s) dive data to look at finer-scale behaviours (on the order of a few hours). My rule of thumb, is that you want 3-50 data points per behaviour. For behaviours spanning several hours, that roughly corresponds to a desired resolution between 2 min and 60 min. 

Let's see what resolutions may be possible in the data by looking at the most frequent time gaps.
```{r}
# identify the most frequent dt
tracks_gps %>% 
  {table(.$dt)} %>% 
  sort(decreasing = T) %>% 
  head()
# visualise time differences
hist(tracks_gps$dt, 1000, main = NA, xlab = "Time difference (min)")
hist(tracks_gps$dt, 1000, main = NA, xlab = "Time difference (min)", xlim = c(0,100))
```
We see that that the most frequent time gap is 10 min, followed by 1, 11, and 12 min. We also see the majority of the gaps are < 60 min, however some are in excess of 600 min. Although it may be possible to use 1 or 2 min resolution data, we will have to address the data gaps. Frequent and large data gaps can be difficult to handle, especially as the number of missing data points approaches or exceeds the existing data; we really want to avoid this. Let's examine the potential data structure at different resolutions for the different animals.
```{r}
# make function to estimate proportion of missing location 
p_na <- function(t_0, t_max, n_loc, resolution) {
  max_n_loc <- length(seq(t_0, t_max, by = as.difftime(resolution, units = "mins")))
  n_NA <- max_n_loc - n_loc
  n_NA / max_n_loc
}

# summarise tracks
 track_summary <-
    tracks_gps %>% st_drop_geometry() %>% 
  # limit to core period with data
  group_by(ID) %>% 
    filter(dt >= 1) %>% # remove duplicate time (just for this stage)
    summarise(n_loc = n(), # number of locations
              p_NA_1m = p_na(first(time), last(time), n_loc, 1),  # 1 min 
              p_NA_2m = p_na(first(time), last(time), n_loc, 2),  # 2 min 
              p_NA_5m = p_na(first(time), last(time), n_loc, 5),  # 5 min 
              p_NA_10m =p_na(first(time), last(time), n_loc, 10),  # 10 min 
              p_NA_20m =p_na(first(time), last(time), n_loc, 20))  # 20 min 
track_summary
```
Here we see that despite the large number of 1 min time steps, at that resolution, >95 /% of the potential locations are missing. Even at the 10 min interval, > 50% of the locations would be missing. Very large data gaps that contribute to much of the missing locations can be excluded from the analysis, therefore, for this tutorial, I will use a 10 min resolution as a compromise between high-resolution information and good temporal coverage.


There are several ways to deal with data gaps, and I will address four 
1. Interpolation (linear or statistical)
2. Multiple imputation
3. Nullification
4. Path segmentation

For large datasets with few and small gaps, the simplest approach to use linear interpolation. First, let's identify thy most likely minute we have data.
```{r}
# which minute has the most data
tracks_gps %>% 
  st_drop_geometry() %>%  # must convert back to data.frame
  group_by(ID) %>% 
  summarise(minute = str_sub(minute(time),-1)) %>% 
  table()
```
It looks like for all three tracks, the most amount of locations fall on 0 min  (i.e., 10, 20, 30, 40, 50, 60 min). Next, for each track, we will create a vector of times in which to estimate locations.

```{r linear interpolation}
# convert tracks back to data.frame with xy coordinates
tracks_gps_list <- tracks_gps %>% 
  mutate(x = st_coordinates(tracks_gps)[,"X"],  
         y = st_coordinates(tracks_gps)[,"Y"]) %>%
  st_drop_geometry() %>% 
    split(.,.$ID)  # split into list

# create full time series on which to estimate locations rounded to the nearest 10 min
tracks_gps_list_time <- tracks_gps %>% 
  st_drop_geometry() %>%  # convert to data.frame
  group_by(ID) %>%
  summarise(time = seq(round_date(first(time), "10 min"), 
                       round_date(last(time), "10 min"),
                       by = 60*10)) %>% 
  split(.,.$ID)  # split into list
```

Now, we can interpolate the locations for each track.

```{r linear interpolation}
# function to create a data frame with approximated locations
approx_locs <- function(tracks, times){
  data.frame(ID = times$ID,
        time = times$time,
        x = approx(tracks$time, tracks$x,
                   xout = times$time)$y,
        y = approx(tracks$time, tracks$y,                               
                   xout = times$time)$y)
}

# Interpolate the location at the times from the sequence
tracks_gps_linear <- mapply(approx_locs, tracks_gps_list, tracks_gps_list_time,
                          SIMPLIFY = F) %>% 
  do.call("rbind", .)  # convert list of tracks back to a single data.frame

# remove row names added by rbind
rownames(tracks_gps_linear) <- NULL 

# plot locations
plot(tracks_gps_linear$x, tracks_gps_linear$y, pch = 20, col = "red", xlab = "x", ylab = "y", asp = 1)
points(st_coordinates(tracks_gps)[,"X"], st_coordinates(tracks_gps)[,"Y"], pch = 20)
```
Looks like it works. Let's try fitting an HMM to this. First, lets preparte the data using `prepData` and plot the data to estimate starting parameters.

```{r linear HMM prep}
prep_gps_linear <- prepData(tracks_gps_linear, type = "UTM")

plot(prep_gps_linear, ask = F)
```


```{r linear HMM starting pars}
# Setting up the starting values
mu0 <- c(50, 500) # Mean step length
sigma0 <- c(50, 500) # Sd of the step length
kappa0 <- c(0.1, 1) # Turning angle concentration parameter (kappa > 0)
```

Ok, were are ready. Let's fit the HMM

```{r linear HMM fitHMM, message=FALSE}
set.seed(1)
# Fit a 2 state HMM
HMM_gps_linear <- fitHMM(prep_gps_linear, nbState = 2, dist = list(step = "gamma",
angle = "vm"), Par0 = list(step = c(mu0, sigma0), angle = kappa0), formula = ~ 1)

plot(HMM_gps_linear, ask = F)
```

That model does not look good. It's clear from the mapped states that state 1 largely represents the original data, while state 2 primarily represents the interpolated data. This is a common problem when data gaps are frequent or large such that the information in the interpolated data outweighs the signal from the original observations. Generally, I would only use linear interpolation when data gaps are small (< 3 locations) or relatively infrequent (< 20 \% of the modelled locations). In our data, some gaps are > 5 hours (30 locations) and > 50\% of the modelled locations are interpolated. So we need to use another approach.

A slightly better way to interpolate locations is to fit a correlated random walk (CRW) model to the data and predict the most likely locations. `momentuHMM` contains wrapper functions to interpolate missing locations by fitting continuous-time CRW to the data based on the `crawl` package by Devin Johnson and Josh London. There are many options to fit the `crawl` model, and a detailed tutorial for analysis with `crawl` is available here: <https://jmlondon.github.io/crawl-workshop/crawl-practical.html>. Let's try to fit the most basic model using the wrapper function `crawlWrap`.

```{r crawl 10 min gps}
# fit crawl
crw_gps_10 <- crawlWrap(obsData = tracks_gps, timeStep = "10 mins")
# plot fit tracks
plot(crw_gps_10, ask = F)
```
Notice how the predicted tracks do not make perfectly straight lines through missing sections (particularly noticeable in T172062). Next, we will extract the predicted locations and add them to the observed data.

```{r crawl predict 10 min}
# filter predicted locations, convert CRS, 
tracks_gps_crw <- data.frame(crw_gps_10$crwPredict) %>% 
  filter(locType == "p") %>% 
  dplyr::select(mu.x, mu.y, time,
         ID) %>% 
  dplyr::rename(x = "mu.x", y = "mu.y")
```

Now, let's try to fit the same HMM as before on this data using the same starting parameters.

```{r crawl HMM fitHMM, message = FALSE}
set.seed(1)
# prep data 
prep_gps_crw <- prepData(tracks_gps_crw, type = "UTM")

# Setting up the starting values
mu0 <- c(50, 500) # Mean step length
sigma0 <- c(50, 500) # Sd of the step length
kappa0 <- c(0.1, 1) # Turning angle concentration parameter (kappa > 0)

# Fit a 2 state HMM
HMM_gps_crw <- fitHMM(prep_gps_crw, nbState = 2, dist = list(step = "gamma",
    angle = "vm"), Par0 = list(step = c(mu0, sigma0), angle = kappa0))

plot(HMM_gps_crw, ask = F)
```
That's looking much better. 



## address small data gaps with simple crawl (<60m)
## address large gaps (either split tracks, or assign SL/TA NA)

## fit crawl model

we can get a better picture of the time differences
```{r}

```

location accuracy (ignoring duplicate locations)
```{r}
# identify proportion of each location class
tracks %>% 
  filter(dt > 0) %>% 
  {table(.$loc_class)/nrow(tracks)} 
```


we have to deal with location error, irregular time, and missing locations. we can handle all of these by fitting a 


```{r}
crawlWrap(tracks, timeStep = "10 min")
```

We will provide a prior distribution for each of the location quality classes. The `crawl::crwMLE()` function accepts a function for the ‘prior’ argument. In this example, we provide a normal distribution of the log-transformed error. The standard error of 0.2
```{r message=FALSE}

prior <- function(p) {
  dnorm(p[1], log(50), 0.2, log = TRUE) +  # GPS
  dnorm(p[2], log(250), 0.2, log = TRUE) +  # 3
  dnorm(p[3], log(500), 0.2, log = TRUE) +  # 2
  dnorm(p[4], log(1500), 0.2, log = TRUE) +  # 1
  dnorm(p[5], log(2500), 0.4, log = TRUE) +  # 0
  dnorm(p[6], log(2500), 0.4, log = TRUE) +  # A
  dnorm(p[7], log(2500), 0.4, log = TRUE) +  # B
  # skip p[8] as we won't provide a prior for sigma
  dnorm(p[9], -4, 2, log = TRUE)  # beta parameter 
}

```
In addition to prior distributions for the location quality classes, we can also provide a prior distribution for the beta parameter. We suggest a normal distribution with a mean of -4 and a standard deviation of 2. This encourages the model to fit a smoother track unless the data warrant a rougher, more Brownian, path.

See the crawl tutorial for using fir
```{r message=FALSE}
# # plot with ggplot
# tracks %>% 
#   mutate(x = st_coordinates(tracks)[,"X"],  # convert back to dataframe
#          y = st_coordinates(tracks)[,"Y"]) %>% 
#   ggplot(aes(
#     x = x, y = y, col = factor(id)), stroke = NA) +
#   geom_path()
# explore palettes 
if (require(shiny) && require(shinyjs)) {
     tmaptools::palette_explorer()
}
```


```{r Create a Nested Data Structure, message=FALSE}
## impute missing locations #

# course speed filter
tracks %>% 
  mutate(spd = sqrt(lag()))
# fit crawl model 
crwOut.win1<- crawlWrap(tracks, ncores = ncores, err.model = err.model,Time.name = "date_time",
            prior = prior, predTime = predTimes_2hour, attempts = 150 , retryFits = 30, proj=6103)
crwOut.win1

i <- 2; error <- T
while(error == T){
  tryCatch({
    i <- i+1
    set.seed(i)
    crwOut <- crawlWrap(obsData = pb, timeStep = "4 hours")  #4 hours
    error <- F
  },
  error = function(e){message(paste("On seed number:", i))}
  )
}


```

```{r Load data, warning=FALSE}
dives <- read.csv("data/dives.csv")
```

```{r plot all tracks}
# convert to lines and plot using tmap
tracks %>% group_by(ID) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING") %>% 
     tm_shape() +
   tm_lines(col = "ID", palette = "Dark2")

```

```{r}
# first passage time
tracks_high <- filter(tracks, loc_class %in% c("GPS", 3, 2, 1))
ltraj <- as.ltraj(xy = cbind(st_coordinates(tracks_high)[,"X"],
                             st_coordinates(tracks_high)[,"Y"]),
                  date = tracks_high$time, id=factor(tracks_high$ID))
fpt_low <- fpt(ltraj, seq(100, 3000, length=100), units = "hours") %>% 
  varlogfpt() 
fpt_med <- fpt(ltraj, seq(3000, 20000, length=200), units = "hours") %>% 
  varlogfpt()
fpt_hig <- fpt(ltraj, seq(20000, 100000, length=200), units = "hours") %>% 
  varlogfpt()
fpt(ltraj, 1500, units = "hours") %>% unlist %>% mean(na.rm = T)/40
fpt(ltraj, 15000, units = "hours") %>% unlist %>% mean(na.rm = T)/40
fpt(ltraj, 40000, units = "hours") %>% unlist %>% mean(na.rm = T)/40
# looks like there are are increases/plateaus of variance at around 1.5, 15, 40, & 80 km
```

