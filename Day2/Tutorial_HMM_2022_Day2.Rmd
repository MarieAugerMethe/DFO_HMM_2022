---
title: "Hidden Markov Models: Missing data and multiple data streams"
editor_options:
  chunk_output_type: console
output:
  html_document:
    number_sections: true
  pdf_document: 
    number_sections: true
---

# Tutorial goals and set up

## Objectives

Our main objectives are to handle some of the common data problems encountered with marine movement and diving data, using a narwhal dataset provided by Dr. Marianne Marcoux. Specifically, we aim to address the following:

-   Irregular locations

-   Time gaps

-   Including diving covariates

# Loading the required packages

First, we'll setup the workspace with required packages.

```{r, message=FALSE}
library(momentuHMM)
library(dplyr)
library(tidyr)
library(lubridate)
library(adehabitatLT)
library(sf)
library(tmap)
library(terra)
library(units)
library(stringr)
library(diveMove)
library(conicfit)
library(car)
library(mitools)
library(doFuture)
library(corrplot)  # Pearson's correlation matrix using cor()
```

<!-- MAM: to be able to compile you need to put all the code in the chunks -->

Please also set working directory to "Day2" of the HMM workshop:

```{r, message=FALSE}
setwd("Day2")
```

# Import data and initial data processing

<!-- MAM: If you onlye use the August data throughout, create a file that has just the August data.  -->
<!-- MAM: also here explain the different data manipulation steps (e.g., NA, location class).  -->
<!-- That's actually quite important in this tutorial about messy data. -->

For simplicity, we will also only look at the data for the month of August, 2017.
```{r import tracks, message=FALSE}
tracks <- read.csv("data/tracks.csv") %>%
  filter(!is.na(x) & !is.na(y)) %>% # remove missing locations
  mutate(
    time = ymd_hms(time), # define time
    loc_class = factor(loc_class, # define location class factor levels
      levels = c("GPS", 3, 2, 1, 0, "A", "B"))) %>% 
  # remove identical records
  filter(!(time == lag(time) & x == lag(x) & y == lag(y) & loc_class == lag(loc_class)),
  # filter only the month of august
  month(time) == 8, day(time) > 7,  day(time) <= 14)

```

The classic HMM assumes the observation data is in discrete time and that there is no missing data in the predictor variables.

<!-- MAM: this code chunk was called calc dt. I've changed to calc_dt. Spaces in names are not good in coding, you want to avoid them. I'm going to change throughout. -->

<!-- MAM : I would consider moving this down to the section selecting a time interval. It will make more sense there. Note that it will affetc the code down the line if you use dt in the Argos analysis. -->

```{r calc_dt}
# Calculate time difference between locations
tracks <- tracks %>%
  mutate(dt = ifelse(ID == lead(ID), # If next data row is same individual
    difftime(lead(time), time, units = "mins"), NA
  )) # calculate time difference
# Look at the frequency of time differences
hist(tracks$dt, 1000, main = NA, xlab = "Time difference (min)")
```

Next, we'll convert the data to a spatial dataset using the `sf` package and plot the data.
First, we define the coordinate reference system of the original data (in this case WGS84, which is defined by the EPSG code `4326`). Next, we will project the data into NAD83(CSRS) UTM zone 21N (EPSG:2962), which will projected the coordinates in meter units with minimal distortion for this data set. 

<!-- MAM: I would recommend using a different object name (tracks_projected). This is a pretty big transformation, and it helps to be able to go back and run stuff on the original object. -->

```{r define_projection, message=FALSE}
tracks <- tracks %>%
  st_as_sf(coords = c("x", "y")) %>% # converts to an sf object
  st_set_crs(4326) %>% # define CRS
  st_transform(2962) # reproject data to a UTM
```

<!-- MAM: I would start here with GPS, and leaving error, and time stamps problems to the Argos part.
So I moved this up, and the filtering stuff down. I note that I'm not sure whether the time stamp issue may cause problems with the GPS data.-->

For the first part of this tutorial, we'll use only the fastloc GPS data so we don't have to deal with location error.

```{r filter_gps_data, message=FALSE}
# filter GPS locations only
tracks_gps <- tracks %>% 
  filter(loc_class=="GPS")
```

We lose some data, particularly near the end of the tracks, but we will integrate ARGOS locations later in this tutorial.

Now, we can map the data using the `tmap` package to visualize what it looks like.

```{r plot_gps_data, message=FALSE}
# plot GPS
tracks_gps %>% 
  group_by(ID) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING") %>% 
     tm_shape() +
   tm_lines(col = "ID", palette = "Dark2")
```


## Selecting a time interval for the HMM

<!-- MAM: this is where I'd think the dt stuff from above would make more sense. Clearly stating that HMM assumes regular time locations, and that marine data is not always at regular time interval is important. -->

There are two key decisions we must make, the temporal resolution to use, and how to address data gaps. The desired resolution depends predominantly on the biological question you are asking as different behaviours and biological processes occur at different spatial and temporal scales (e.g. seasonal migration, daily movement between foraging and resting grounds, and fine scale foraging decisions). Generally, higher resolution data is preferred as it has more information, however it is possible to have too-high of a resolution wherein information from fine-scale variability drowns out the signal from coarse-scale patterns of interest (e.g., seasonal migration). In this case, we will be linking the movement data with high resolution (75 s) dive data to look at finer-scale behaviours (on the order of a few hours). My rule of thumb, is that you want 3-50 data points per behaviour. For behaviours spanning several hours, that roughly corresponds to a desired resolution between 2 min and 60 min. 



Let's see what resolutions may be possible in the data by looking at the most frequent time gaps.
```{r calc_track_dt}
# identify the most frequent dt
tracks_gps %>% 
  {table(.$dt)} %>% 
  sort(decreasing = TRUE) %>% 
  head()
```


```{r view_track_dt}
# Visualize time differences
hist(tracks_gps$dt, 1000, main = NA, xlab = "Time difference (min)")
# Zoom in on short intervals
hist(tracks_gps$dt, 1000, main = NA, xlab = "Time difference (min)", xlim = c(0,100))
```

We see that that the most frequent time gap is 10 min, followed by 11, 1, 9, and 12 min. We also see the majority of the gaps are < 60 min, however some are in excess of 600 min. Although it may be possible to use 1 or 2 min resolution data, we will have to address the data gaps. Frequent and large data gaps can be difficult to handle, especially as the number of missing data points approaches or exceeds the existing data; we really want to avoid this. Let's examine the potential data structure at different resolutions for the different animals.

We first create a function that can calculate the proportion of missing locations we would have for a given resolution.

```{r proportion_NA_fx}
# Make function to estimate proportion of missing location 
p_na <- function(t_0, t_max, n_loc, resolution) {
  max_n_loc <- length(seq(t_0, t_max, by = as.difftime(resolution, units = "mins")))
  n_NA <- max_n_loc - n_loc
  n_NA / max_n_loc
}
```

We can now use this function look at the proportion of NAs we would get with 1, 2, 5, 10, and 20 min resolution.

```{r track_resolution_proportion_NA}
# summarise tracks
track_summary <-
    tracks_gps %>% st_drop_geometry() %>% 
  # limit to core period with data
  group_by(ID) %>% 
    filter(dt >= 1) %>% # remove duplicate time (just for this stage)
    summarise(n_loc = n(), # number of locations
              p_NA_1m = p_na(first(time), last(time), n_loc, 1),  # 1 min 
              p_NA_2m = p_na(first(time), last(time), n_loc, 2),  # 2 min 
              p_NA_5m = p_na(first(time), last(time), n_loc, 5),  # 5 min 
              p_NA_10m =p_na(first(time), last(time), n_loc, 10),  # 10 min 
              p_NA_20m =p_na(first(time), last(time), n_loc, 20))  # 20 min 
track_summary
```
Here we see that despite the large number of 1 min time steps, at that resolution, >95 /% of the potential locations are missing. Even at the 10 min interval, > 50% of the locations would be missing. Very large data gaps that contribute to much of the missing locations can be excluded from the analysis, therefore, for this tutorial, I will use a 10 min resolution as a compromise between high-resolution information and good temporal coverage.

## Handling data gaps

There are several ways to deal with data gaps, and I will address four 
1. Interpolation (linear or statistical)
2. Nullification
3. Path segmentation
4. Multiple imputation

### Approach 1. Interpolation

For large datasets with few and small gaps, the simplest approach to use linear interpolation. First, let's identify the most likely minute we have data.
```{r identify_most_freq_dt}
# which minute has the most data
tracks_gps %>% 
  st_drop_geometry() %>%  # must convert back to data.frame
  group_by(ID) %>% 
  summarise(minute = str_sub(minute(time),-1)) %>% 
  table()
```

It looks like for all three tracks, the most amount of locations fall on 0 min  (i.e., 10, 20, 30, 40, 50, 60 min). Next, for each track, we will create a vector of times in which to estimate locations.

```{r linear_interpolation_prep}
# convert tracks back to data.frame with xy coordinates
tracks_gps_ls <- tracks_gps %>% 
  mutate(x = st_coordinates(tracks_gps)[,"X"],  
         y = st_coordinates(tracks_gps)[,"Y"]) %>%
  st_drop_geometry() %>% 
    split(.,.$ID)  # split into list

# create full time series on which to estimate locations rounded to the nearest 10 min
tracks_gps_ls_time <- tracks_gps %>% 
  st_drop_geometry() %>%  # convert to data.frame
  group_by(ID) %>%
  summarise(time = seq(round_date(first(time), "10 min"), 
                       round_date(last(time), "10 min"),
                       by = 60*10)) %>% 
  split(.,.$ID)  # split into list
```

Now, we can interpolate the locations for each track.

<!-- MAM: you can't have two code chunk with the same name. You won't be able to compile it. So change name above. -->

```{r linear_interpolation}
# function to create a data frame with approximated locations
approx_locs <- function(tracks, times){
  data.frame(ID = times$ID,
        time = times$time,
        x = approx(tracks$time, tracks$x,
                   xout = times$time)$y,
        y = approx(tracks$time, tracks$y,                               
                   xout = times$time)$y)
}

# Interpolate the location at the times from the sequence
tracks_gps_linear <- mapply(approx_locs, tracks_gps_ls, tracks_gps_ls_time,
                          SIMPLIFY = F) %>% 
  do.call("rbind", .)  # convert list of tracks back to a single data.frame

# remove row names added by rbind
rownames(tracks_gps_linear) <- NULL 

# plot locations
plot(tracks_gps_linear$x, tracks_gps_linear$y, pch = 20, col = "red", xlab = "x", ylab = "y", asp = 1)
points(st_coordinates(tracks_gps)[,"X"], st_coordinates(tracks_gps)[,"Y"], pch = 20)
```

Looks like it works. Let's try fitting an HMM to this. First, lets prepare the data using `prepData` and plot the data to estimate starting parameters.

```{r linear_HMM_prep}
prep_gps_linear <- prepData(tracks_gps_linear, type = "UTM")

plot(prep_gps_linear, ask = F)
```


```{r linear_HMM_starting_pars}
# Setting up the starting values
mu0 <- c(50, 500) # Mean step length
sigma0 <- c(50, 500) # Sd of the step length
kappa0 <- c(0.1, 1) # Turning angle concentration parameter (kappa > 0)
```

Ok, were are ready. Let's fit the HMM

```{r linear_HMM_fitHMM, message=FALSE}
set.seed(1)
# Fit a 2 state HMM
HMM_gps_linear <- fitHMM(prep_gps_linear, nbState = 2, dist = list(step = "gamma",
angle = "vm"), Par0 = list(step = c(mu0, sigma0), angle = kappa0), formula = ~ 1)

plot(HMM_gps_linear, ask = FALSE)
```

That model does not look good. It's clear from the mapped states that state 1 largely represents the original data, while state 2 primarily represents the interpolated data. This is a common problem when data gaps are frequent or large such that the information in the interpolated data outweighs the signal from the original observations. Generally, I would only use linear interpolation when data gaps are small (< 3 locations) or relatively infrequent (< 20 \% of the modelled locations). In our data, some gaps are > 5 hours (30 locations) and > 50\% of the modelled locations are interpolated. So we need to use another approach.

A slightly better way to interpolate locations is to fit a correlated random walk (CRW) model to the data and predict the most likely locations. `momentuHMM` contains wrapper functions to interpolate missing locations by fitting continuous-time CRW to the data based on the `crawl` package by Devin Johnson and Josh London. There are many options to fit the `crawl` model, and a detailed tutorial for analysis with `crawl` is available here: <https://jmlondon.github.io/crawl-workshop/crawl-practical.html>. Let's try to fit the most basic model using the wrapper function `crawlWrap`.

```{r crawl_10_min_gps}
set.seed(1) # crawl often fails, so I recommend always setting a seed 
# fit crawl
crw_gps_10 <- crawlWrap(obsData = tracks_gps, timeStep = "10 mins")
# plot fit tracks
plot(crw_gps_10, ask = F)
```
Notice how the predicted tracks do not make perfectly straight lines through missing sections (particularly noticeable in T172062). Next, we will extract the predicted locations and add them to the observed data.

```{r crawl_predict_10_min}
# filter predicted locations
tracks_gps_crw <- data.frame(crw_gps_10$crwPredict) %>% 
  filter(locType == "p") %>% 
  dplyr::select(mu.x, mu.y, time,
         ID) %>% 
  dplyr::rename(x = "mu.x", y = "mu.y")
```

Now, let's try to fit the same HMM as before on this data using the same starting parameters.

```{r crawl_HMM_fitHMM, message=FALSE}
set.seed(1)
# prep data 
prep_gps_crw <- prepData(tracks_gps_crw, type = "UTM")

# Setting up the starting values
mu0 <- c(50, 500) # Mean step length
sigma0 <- c(50, 500) # Sd of the step length
kappa0 <- c(0.1, 1) # Turning angle concentration parameter (kappa > 0)

# Fit a 2 state HMM
HMM_gps_crw <- fitHMM(prep_gps_crw, nbState = 2, dist = list(step = "gamma",
    angle = "vm"), Par0 = list(step = c(mu0, sigma0), angle = kappa0))

plot(HMM_gps_crw, ask = F)
```

That's looking much better. It looks like state 1 represents a low-speed, high tortuosity resident state, while state 2 reperesents higher-speed, low tortuosity travelling state. In many instances, this model may be sufficient. However, because we do have such a large amount of interpolated locations, they represent a significant amount of the fitted data and may skew the representation of the states. For example, the large interpolated gaps are still relatively straightened out and a very consistent speed, and may skew the definition of state 2 in particular. There are different ways to deal with this, one of which is to simply keep it in mind when interpreting the results and subsequent analyses. However, here I propose one strategy by nullifying the data streams (i.e., step length and turning angle) during moderate/large interpolated gaps where we expect that the estimated movement has is largely independent of the observed data before or after the gap. The maximum size of a gap to allow depends on the frequency of the missing data, frequency of locations, study species, and behaviours of interest. In this case, I will nullify predicted locations in gaps larger than 60 min. First, we will identify which steps need to be nullified, then we will prepare the data and nullify the estimated `step` and `angle` datastreams. We will do this again later in the tutorial, so we will wrap it into a function called `prepData_NAGaps`.

### Approach 2. Nullify data gaps

```{r id_nullify_gaps}
# define function to replace step and turn angle of large gaps with NA
NA_gaps <- function(tracks, times){
  # rows where location is within a large gap
  rows <- which(
    rowSums(apply(times, 1, function(X, tracks){
      dplyr::between(tracks, 
                     as.POSIXct(X[1], tz = "UTC"),
                     as.POSIXct(X[2], tz = "UTC"))
    }, tracks$time))==1)
  tracks$step[rows] <- NA
  tracks$angle[rows] <- NA
  return(tracks)
}

# define function to identify and nullify gaps
prepData_NAGaps <- function(track_list, tracks_crw, res, max_gap, ...){
  # for each ID, identify which rows have gaps >= max_gap 
  gaps_ls_rows <- lapply(track_list, function(x){
    which(difftime(lead(x$time), x$time, units = "mins") >= max_gap)
  })
  
  # create sequence of times for each track from gaps >= 60 min
  gap_ls <- mapply(FUN = function(track, gaps){
    # identify start and end date of each gap
    gaps_ls_srt_end <- list(start = ceiling_date(track$time[gaps], paste(res, "min")),
                            end = floor_date(track$time[gaps+1], paste(res, "min")))
    # combine into single vector for each track
    data.frame(start = gaps_ls_srt_end$start, end = gaps_ls_srt_end$end)
  },
  track_list, gaps_ls_rows, SIMPLIFY = F)
  
  # prep data and list by ID
  prep_tracks <- prepData(tracks_crw, ...) %>% 
    {split(., .$ID)}
  
  # Interpolate the location at the times from the sequence
  mapply(FUN = NA_gaps, prep_tracks, gap_ls,
         SIMPLIFY = F) %>% 
    do.call("rbind", .) # convert list of tracks back to a single data.frame
}
  
prep_tracks_gps_crw_NAgaps <- prepData_NAGaps(tracks_gps_ls, tracks_gps_crw, 10, 60, type = "UTM")
```

Now, let's try to fit the same HMM as above to this data with large gaps nullified. 

```{r nullified_crawl_HMM_fitHMM, message=FALSE}
set.seed(1)
# Setting up the starting values
mu0 <- c(50, 500) # Mean step length
sigma0 <- c(50, 500) # Sd of the step length
kappa0 <- c(0.1, 1) # Turning angle concentration parameter (kappa > 0)

# Fit a 2 state HMM
HMM_gps_crw_NAgaps <- fitHMM(prep_tracks_gps_crw_NAgaps, nbState = 2, dist = list(step = "gamma", angle = "vm"), Par0 = list(step = c(mu0, sigma0), angle = kappa0))

plot(HMM_gps_crw_NAgaps, ask = F)
```
Visually, the difference is subtle.

```{r}
HMM_gps_crw$mle[c("step", "angle")]
HMM_gps_crw_NAgaps$mle[c("step", "angle")]
```
The estimated parameters are quite different whether you account for the large gaps in data. When you nullify large gaps, the mean step length for both states is higher, and the turn angle concentration parameters is lower for both states (i.e., more tortuous). The fact that the parameters change for both states, suggests that the large gaps skewed the parameterisation of both states.

Another strategy to deal with larger gaps is to segment the tracks with a new individual ID. This may be particularly appropriate for gaps where we may reasonably expect that the the underlying states are effectively independent of one another. Specifically, we may ask, over what period of time does the behaviour of the animal affect subsequent behaviour. In this case, we may expect that the behaviour of a narwhal depends on the behaviour over the proceeding several hours, however is independent after 24 hours. We can split the tracks for gaps larger than a predetermined threshold by iterating the the ID column. We will not implement this approach in this tutorial, however, it can be done using the following code:

```{r track segmentation}
gap_thresh <- 3*60 # in hours (3h for illustration)
# gaps no more than 6h per segment
new_ID <- (tracks_gps$dt > gap_thresh | tracks_gps$ID != lag(tracks_gps$ID)) %>%  # if dif.time > gap_thresh or new ID
  replace_na(TRUE) %>%  # replace first NA with ID = 1
  cumsum() %>%  # iterate ID 
  paste(tracks_gps$ID, ., sep = "_")
# then smaller gaps <= gap_thresh can be interpolated with crawlWrap
```

## Dealing with location error and irregular time
Thus far, we have been using exclusively the GPS data, which can be assumed to have a negligible error in most applications. However, in many marine systems, we may obtain ARGOS locations, which can have highly variable location error on the order of hundreds of meters to hundreds of km. In addition, ARGOS data often provides location estimates in very irregular time intervals. `crawlWrap` can be used not only to predict impute missing data, but also to predict location when faced with high location error and irregular time-series. 

<!-- MAM: I moved this from above. But didn't quite check if it' integrated well yet. -->
Visualize original data that includes Argos location.

```{r plot original data, message=FALSE}
# convert to lines and plot using tmap
tracks %>% 
  group_by(ID) %>% 
  summarise(do_union = FALSE) %>% 
  st_cast("LINESTRING") %>% 
  tm_shape() + 
  tm_sf(col = "ID", palette = "Dark2")
```

It looks like there are at least several outlier points. We can address most of these using a coarse speed filter. However, before we can calculate speed ($\frac{\Delta dist}{\Delta t}$), we must address the duplicate time-stamps (since we cannot divide by 0 to calculate speed). It is often not possible to reliably identify which record is more accurate to discard. Therefore, it's best to retain as as much data and offset duplicate times slightly. We will first offset duplicate times by adding 10s for each consecutive row with a `dt == 0`.

<!-- MAM: why 10 sec? Is there are reason not to go to one sec? -->

```{r offset_duplicate_times, message=FALSE}
# 1) count the run length of each time step using rle
run_length <- rle(tracks$dt)$lengths
# 2) create a sequence of 1:run_length for each time step
run_seqence <- sequence(run_length)
# 3) discount each time step that's not a duplicate by multiplying by 0
run_seqence <- run_seqence*(tracks$dt==0)
# 4) lag run_sequence by one row and replace the first value by 0,
# so that we iterate each time that is a duplicate of the previous row
tracks$duplicate_count <- replace_na(lag(run_seqence), 0)
# offset duplicate times by 10s
tracks <- 
  tracks %>%
  mutate(time = time + 10*duplicate_count,
         # recalculate time difference
         dt = ifelse(ID == lead(ID), 
    difftime(lead(time), time, units = "mins"), NA
  ))

# last we ensure there are no identical times
sum(tracks$time == lead(tracks$time), na.rm = TRUE)
```

<!-- MAM: why are you using a speed filter? Aren't you using crawl to handle the error? -->

Now, we can apply a speed filter to remove locations that are faster than some threshold -- in this case we will exclude any locations $>10$ m/s from the previous location.

```{r speed_filter, message=FALSE}
# distance from previous location 
delta_dist <- st_distance(tracks, lag(tracks), by_element=TRUE)
# time from previous location
delta_t <- set_units(lag(tracks$dt), "min") 
# calculate speed
tracks$spd <- delta_dist/delta_t

# filter speed
tracks <- tracks %>% 
  filter(spd < set_units(10, "m/s"))

# convert to lines and plot
tracks %>% group_by(ID) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING") %>% 
     tm_shape()+
   tm_lines(col = "ID", palette = "Dark2")
```

Already looking much better.


location accuracy (ignoring duplicate locations)
```{r}
# identify proportion of each location class
tracks %>% 
  filter(dt > 0) %>% 
  {table(.$loc_class)/nrow(tracks)} 
```

we have to deal with location error, irregular time, and missing locations. The `crawl` model that we fit earlier can be extended to predict tracks from irregular data with location error. In this section, we will cover using `crawlWrap` to amputate regular locations when faced with more difficult data (e.g., ARGOS) and fitting an HMM to multiple imputed data.

First let's identify a resolution to work at if only obtained ARGOS data.

```{r select ARGOS resolution}
# filter argos data
tracks_argos <- filter(tracks, loc_class != "GPS") %>% 
  # calculate dime difference
  mutate(dt = ifelse(ID == lead(ID),
    difftime(lead(time), time, units = "mins"), NA))

# split ARGOS tracks by ID for later steps
tracks_argos_ls <- tracks_argos %>% 
  mutate(x = st_coordinates(tracks_argos)[,"X"],  
         y = st_coordinates(tracks_argos)[,"Y"]) %>%
  st_drop_geometry() %>% 
    split(.,.$ID)

# visualise time differences
hist(tracks_argos$dt[tracks_argos$dt<180], 32, 
     main = NA, xlab = "Time difference (min)")
```
In this section, we will use 45 min as a trade-off between higher resolution and minimising data gaps.

Next, we will define model structure for `crawl`. The `crawl` tutorial suggests that users define prior distributions to optimize the fit and increase the model’s ability to converge with limited/challenging data while providing the model flexibility. Here, we provide a normal distribution of the log-transformed error for each of the ARGOS error classes. The tutorial also suggest users rely on the prior distribution for the beta parameter centered on -4 (smoother fit) and, if needed, fix the beta parameter to -4 (see `crawl` tutorial [HERE](https://jmlondon.github.io/crawl-workshop/crawl-practical.html#predicting-a-movement-track)).
```{r fit 45 min crawl to argos and gps tracks,  message=FALSE}
# define priors
prior <- function(p) {
  dnorm(p[1], log(250),  0.2, log = TRUE) +  # 3
  dnorm(p[2], log(500),  0.2, log = TRUE) +  # 2
  dnorm(p[3], log(1500), 0.2, log = TRUE) +  # 1
  dnorm(p[4], log(2500), 0.4, log = TRUE) +  # 0
  dnorm(p[5], log(2500), 0.4, log = TRUE) +  # A
  dnorm(p[6], log(2500), 0.4, log = TRUE) +  # B
  # skip p[7] as we won't provide a prior for sigma
  dnorm(p[8], -3, 3, log = TRUE)  # beta parameter 
}

# define error model
err.model <- list(x = ~ loc_class - 1, y = ~ loc_class - 1)

# define times to predict for easier comparison between ARGOS and GPS tracks
predTime <- lapply(tracks_argos_ls, function(x)
  seq(first(x$time), last(x$time), "45 min"))

# fit crawl model and predict locations at 45 min time steps
set.seed(1)
argos_45_crwOut <- crawlWrap(tracks_argos,
  timeStep = "45 min",
  retryFits = 1, Time.name = "time",
  err.model = err.model,
  prior = prior, 
  predTime = predTime) 
plot(crw_argos_45, ask = F)
# convert to data.frame and extract predicted locations
crw_argos_45 <- data.frame(argos_45_crwOut$crwPredict) %>% 
  filter(locType == "p") %>% 
  dplyr::select(mu.x, mu.y, time, ID) %>% 
  dplyr::rename(x = "mu.x", y = "mu.y") %>% 
  {.[-1,]}  # remove first data record as it is not present in the GPS data 

# for comparison, we will also interpolate the GPS data to 45 min
set.seed(1)
crw_gps_45 <- crawlWrap(obsData = tracks_gps, timeStep = "45 mins", 
                        predTime = predTime) %>% 
  # convert to data.frame and extract predicted locations
  {data.frame(.$crwPredict)} %>% 
  filter(locType == "p") %>% 
  dplyr::select(mu.x, mu.y, time, ID) %>% 
  dplyr::rename(x = "mu.x", y = "mu.y")

# compare estimated tracks
filter(crw_argos_45, ID=="T172062")  %>% 
plot(y~x, data=., type = 'l', col = 'red', asp=1) 
filter(crw_gps_45, ID=="T172062") %>% 
points(y~x, data=., type = 'l')
```
We see that there is pretty good over lap between the ARGOS (red) and GPS (black) tracks at a 45 min resolution. However, there are some sections where the ARGOS track cuts straight lines through sections where there is variability in the GPS track.

Let's try to fit an HMM to the interpolated ARGOS tracks.
```{r prep ARGOS tracks and fit basic HMM, message = FALSE}
set.seed(1)
# prep tracks
prep_argos_45 <- prepData(crw_argos_45, type = "UTM")
prep_gps_45 <- prepData(crw_gps_45, type = "UTM")
  
# setup model
dist <- list(step = "gamma", angle = "vm")
# Setting up the starting values
mu0 <- c(1500, 3500) # mu step
sigma0 <- mu0/2      # Sd step
kappa0 <- c(1, 4)    # kappa angle
Par0 <- list(step = c(mu0, sigma0), angle = kappa0)

# Fit a 2 state HMM for 45 min ARGOS and GPS tracks
HMM_argos_45_basic <- fitHMM(prep_argos_45, nbState = 2, dist = dist, Par0 = Par0)
HMM_gps_45_basic <- fitHMM(prep_gps_45, nbState = 2, dist = dist, Par0 = Par0)

plot(HMM_argos_45_basic, animals = "T172062", ask = F)
plot(HMM_gps_45_basic, animals = "T172062", ask = F)
# compare predicted states
sum(viterbi(HMM_argos_45_basic) == viterbi(HMM_gps_45_basic))/nrow(prep_argos_45)
```
The interpolated GPS track seems to fit, however the model had some issues fitting the ARGOS data. Further, $<50/%$ of the predicted states from the ARGOS model are the same as the GPS model. I suspect the issue may be due to the large gaps where the interpolated track is basically linear. Indeed, in the GPS HMM, state 1 seems to primarily be classifying the straight interpolated sections as state 1 and the rest as state 2. We can try to address this by nullify gaps using the `prepData_NAGaps` function we defined earlier. In this case, we will nullify gaps $>180$ min
```{r prep data and nullify gaps}
# prep 45 min ARGOS data with nullified gaps
prep_argos_45 <- prepData_NAGaps(track_list = tracks_argos_ls, tracks_crw = crw_argos_45, res = 45, max_gap = 180, type = "UTM")

# prep 45 min gps data with nullified gaps
prep_gps_45 <- prepData_NAGaps(track_list = tracks_gps_ls, tracks_crw = crw_gps_45, res = 45, max_gap = 180, type = "UTM")
```
Now, let's try to fit the same HMM as above to this data with large gaps nullified. 

```{r nullified crawl HMM fitHMM, message = FALSE}
set.seed(1)
# Fit a 2 state HMM for 45 min ARGOS and GPS tracks
HMM_argos_45 <- fitHMM(prep_argos_45, nbState = 2, dist = dist, Par0 = Par0)
HMM_gps_45 <- fitHMM(prep_gps_45, nbState = 2, dist = dist, Par0 = Par0)

plot(HMM_argos_45, animals = "T172062", ask = "F")
plot(HMM_gps_45, animals = "T172062", ask = "F")
# compare new and original GPS model predicted states
sum(viterbi(HMM_gps_45_basic) == viterbi(HMM_gps_45))/nrow(prep_argos_45)
# compare new and original GPS model predicted states
sum(viterbi(HMM_argos_45) == viterbi(HMM_gps_45))/nrow(prep_argos_45)
```
Both data sets seem to fit easily. There isn't a large change between this GPS model and the previous one (without nullifying large gaps); $91/%$ of the predicted states are the same. However, the overlap between the ARGOS and GPS models is much higher than previously ($71\%$), suggesting the ARGOS model with nullified gaps is significantly improved. However, there is still a notable discrepancy between the ARGOS and the GPS HMMs.

### Multiple imputation
```{r fit multiple imputations}

# define priors
prior <- function(p) {
  dnorm(p[1], log(250),  0.2, log = TRUE) +  # 3
  dnorm(p[2], log(500),  0.2, log = TRUE) +  # 2
  dnorm(p[3], log(1500), 0.2, log = TRUE) +  # 1
  dnorm(p[4], log(2500), 0.4, log = TRUE) +  # 0
  dnorm(p[5], log(2500), 0.4, log = TRUE) +  # A
  dnorm(p[6], log(2500), 0.4, log = TRUE) +  # B
  # skip p[7] as we won't provide a prior for sigma
  dnorm(p[8], 4, 2, log = TRUE)  # beta parameter 
}

# fit crawl model and predict locations at 45 min time steps
set.seed(1)
argos_45_crwOut <- crawlWrap(tracks_argos,
  timeStep = "45 min",
  retryFits = 1, Time.name = "time",
  err.model = err.model,
  prior = prior, 
  predTime = predTime) 


nfsFits <- MIfitHMM(argos_45_crwOut, nSims = 1, fit = F)
nfsFits <- MIfitHMM(argos_45_crwOut, nSims = 1, nbStates = 2, dist = dist, Par0 = Par0)
plot(nfsFits)
set.seed(1)

track_T172066 <- tracks_argos %>% 
  filter(ID == "T172066")
argos_45_crwOut_foo1 <- track_T172066 %>% 
  filter(loc_class %in% c("3","2", "0", "A")) %>% 
           crawlWrap(timeStep = "45 min",
  retryFits = 1, Time.name = "time",
  err.model = err.model,
  prior = prior, 
  predTime = predTime$T172066[which(track_T172066$loc_class %in% c("3","2", "0", "A"))]) 

nfsFits <- MIfitHMM(argos_45_crwOut_foo1, nSims = 1, fit = F)



gps_45_crwOut <- crawlWrap(tracks_gps,
  timeStep = "45 min", Time.name = "time",
  predTime = predTime)
gpsFits <- MIfitHMM(gps_45_crwOut, nSims = 1, fit = F)
```

## Integrating dive data

```{r import depth data, warning=FALSE}
dives <- read.csv("data/dives.csv") %>% 
  mutate(time = ymd_hms(time)) %>% 
  filter(month(time) == 8, day(time) > 7,  day(time) <= 14)

head(dives)
table(dives$dt)
```

It appears as though there are relatively few gaps, however the gaps that exist are relatively long (> 60 mins). Therefore, I suggest that we should regularize and nullify the dive data.

```{r regularise depth data, warning=FALSE}
# regularise data 
dives <- dives %>% 
  group_by(ID) %>%
  # regularise time series by 1.25 min
  summarise(time = seq(first(time), last(time), by = 1.25*60)) %>%
  # merge regularised time with original dive
  left_join(dives, by = c("ID", "time"))
```


Next, we have to summarise the time series into concrete data streams that can be modelled in the HMM. We can use the `diveMove` package to identify individual dives and calculate dive statistics. We must first convert the depth data to the `TDR` class for each whale. As we are working with multiple whales, we will use `lapply` to apply the `createTDR` to each whale -- we will also use `lapply` for most of `diveMove` functions. 

```{r convert depth data to TDR, warning=FALSE}
dives_ls <- split(dives, dives$ID) 
dive_TDR <- lapply(dives_ls, function(data){
  createTDR(time = data$time, depth = data$depth, dtime = 1.25*60, file = "data/dives.csv")
  })

# generate interactive plot
plotTDR(dive_TDR[[1]]) # note: try zooming in to part of the dive
```

Next, we must use the `calibrateDepth` to calibrate the depth data. This function identifies wet and dry periods (for animals that haul out), applies a zero-offset correction (ZOC), and identifies all dives in the record and their phases. ZOC can be done using either the `offset` or `filter` method. In this case, we will assume the depth data is accurate and does not require an offset. We should also specify `dive.thr`, which represents the threshold depth below which an underwater phase should be considered a dive -- in this case, we will set it at 8 m. There are many other optional parameters to identify dives that we will not get into in this tutorial (See `vignette("diveMove")` for details). 

```{r calibrate TDR, warning=FALSE}
# calibrate TDR and identify dive phases
dive_TDR_calib <- lapply(X = dive_TDR, FUN = calibrateDepth, zoc.method = "offset", offset = 0, dive.thr = 8)

# interactive plot of calibrated DTR
plotTDR(dive_TDR_calib[[1]], surface = T)
```
In the interactive plot, try zooming into one of the dives and hover mouse over plot to preview the phases identified by `calibrateDepth'. The phases identified correspond to descent (D), descent/bottom (DB), bottom (B), bottom/ascent (BA), ascent (A), and surface (X). We can plot a specific dives as follows:
```{r plot TDR_calib dives}
plotTDR(dive_TDR_calib[[1]], diveNo = 1:300, surface = T)
```
Now, we can calculate summary statistics for each dive using the function `diveStats`. There are many dive metrics that are estimated, and which ones to retain are species, data, and question specific. In this case, we will retain 8 from those calculated by `diveStats`: dive time, bottom time,  maximum depth, bottom distance (measure of "wiggling while at the bottom), post-dive duration.

```{r calc summary dive stats}
# calculate dive stats and add dive.id to each dive 
dive_sum <- lapply(dive_TDR_calib, function(data){
  mutate(diveStats(data, depth.deriv = F), dive.id = row_number()) %>% 
    dplyr::select(dive.id, divetim, botttim, maxdep, bottdist, postdive.dur)}) # select variables of interest

# add dive.id with depth data to each depth record
dives_ls <- mapply(function(TDR, dives){
  mutate(TDR, dive.id = dives@dive.activity$dive.id)
  }, TDR = dives_ls, dives = dive_TDR_calib, SIMPLIFY = F)

# join TDR data with dive summary data
dives_ls <- mapply(function(TDR, dive_sum){
  left_join(TDR, dive_sum, by = "dive.id")
  }, TDR = dives_ls, dive_sum = dive_sum, SIMPLIFY = F)

# convert dive_ls back to a data.frame
dives_df <- do.call(rbind, dives_ls)
```
Next, we will replace any bottom time of a valid dive (dive.id > 0) to 75 s, since at least some time must be spent at the bottom. Then, we will calculate one additional metric as a proxy for dive shape; the ratio of bottom time to dive time. Dives where $\leq20\%$ of the time is spent at the bottom represent V-shaped dives, U-shaped dives are represented when $>20$ and $\leq50\%$ is spent at the bottom, and square dives are represented when $>50\%$ of the time is spent at the bottom. 
```{r calc percent bottom}
# replace NA bottom time of valid dives
dives_df$botttim[dives_df$dive.id > 0 & is.na(dives_df$botttim)] <- 75
# calculate proportion time at bottom
dives_df <- dives_df %>% 
  mutate(propbott = botttim/divetim)
# remove "dives" with no duration
dives_df <- dives_df %>% 
  filter(!(dive.id > 0 & is.na(divetim)))
```
The next issue is that the dive data is at a different temporal resolution (75 s) than the location data (10 min). There are two options to include both data streams in the same HMM. First, we can choose to implement a hierarchical HMM, however, this is more complicated, and  will be covered in tomorrow's tutorial. The second, which we will use here, is to summarise the depth/dive data to a 10 min resolution and include them as additional data streams with step length and turning angle. 
```{r summarise dive data to 10 min}
dives_sum <- dives_df %>% 
  # round time to same interval as location data
  mutate(time = floor_date(time, "10 min")) %>% 
  # group rows by time interval
  group_by(ID, time) %>% 
  # summarise data
  summarise(NA_t          = sum(is.na(depth))*1.25,
            surf_t        = sum(dive.id == 0)*1.25,
            mean_dep      = ifelse(NA_t == 10, NA, mean(na.rm = T, depth)),
            max_dep       = ifelse(NA_t == 10, NA, max(na.rm = T, depth)),
            dive_t        = ifelse(NA_t == 10, NA, sum(na.rm = T, divetim)),
            bott_t        = ifelse(dive_t < 5, NA, sum(na.rm = T, botttim)),
            prop_bott     = ifelse(dive_t < 5, NA, max(na.rm = T, propbott)),
            max_dive_dep  = ifelse(dive_t < 5, NA, max(na.rm = T, maxdep)),
            bott_dist     = ifelse(dive_t < 5, NA, max(na.rm = T, bottdist)),
            post_dive_dur = ifelse(dive_t < 5, NA, max(na.rm = T, postdive.dur))) %>% 
  # remove -Inf values (typically error)
  filter(!is.infinite(dive_t) & !is.infinite(bott_dist))

# preview
head(dives_sum)
```
Note, we used 2 different `ifelse` statements. First, `ifelse(NA_t == 10, ...)` ensured that we only calculate mean and maximum depth for periods where we have at least 1.25 min of depth data. Second, `ifelse(surf_t > 5, ...)` ensured that we only calculate dive metrics when at least half of the 10 min interval is spent in a dive, otherwise the animal is assumed to be at the surface and its dive metric is `NA`.

Any of the eight variables can be used as data streams in the HMM, however, including too many would significantly increase the number of parameters to estimate, and consequently computation time. Therefore, we must select which variables to use, which there are different several approaches:
1. Variable can be selected using expert on the species behaviour and research question.
2. We preferably want to avoid variable with a lot of missing data. 
3. The data streams should exhibit variation and evidence of clustering or multi-modality that may be tied to the underlying behaviours. 
4. Data streams with no variation or conversely are very noisy contain little information on underlying behaviour.
5. We want to avoid variables that are overdispersed and which would be difficult to describe using a statistical distribution. 
6. Given that they are tied to autocorrelated behaviours, they too should exhibit some -- but not too much -- autocorrelation. 
7. One of HMM's key assumptions that the data stream are be independent of each other, therefore, we should avoid select highly co-linear data streams. 

Assuming all the variables are biologically relevant, let's look at structure in the data.
I will also add a subjective score based on the results (first number after `#` symbol).

```{r data structure}
vars <- c("mean_dep","max_dep","dive_t","bott_t","prop_bott",
          "max_dive_dep","bott_dist","post_dive_dur")
# First, missing data
dives_sum %>% 
  summarise(mean_dep      = sum(is.na(mean_dep)),      # 2
            max_dep       = sum(is.na(max_dep)),       # 2
            dive_t        = sum(is.na(dive_t)),        # 2
            bott_t        = sum(is.na(bott_t)),        # 1
            prop_bott     = sum(is.na(prop_bott)),     # 1
            max_dive_dep  = sum(is.na(max_dive_dep)),  # 1
            bott_dist     = sum(is.na(bott_dist)),     # 1
            post_dive_dur = sum(is.na(post_dive_dur))) # 1

# Second, evidence of multi-modality, balanced variation, and no over-dispersion
# as the dive variables are zero-bound, applying a log transformation makes it easier to see structure in the distirbution 
# dives_sum %>% 
#   ungroup() %>% 
#   dplyr::select(vars) %>% 
#   apply(2, hist, 100)
hist(log(dives_sum$mean_dep), 100)      # 2 good structure, high dispersion
hist(log(dives_sum$max_dep), 100)       # 3 great structure, moderate dispersion
hist(log(dives_sum$dive_t), 100)        # 1 little structure, fragmented distribution
hist(log(dives_sum$bott_t), 100)        # 0 little structure, fragmented distribution
hist(logit(dives_sum$prop_bott), 100)   # 0 no structure, fragmented distribution
hist(log(dives_sum$max_dive_dep), 100)  # 2 good structure, low dispersion
hist(log(dives_sum$bott_dist), 100)     # 2 moderate structure, high dispersion
hist(log(dives_sum$post_dive_dur), 100) # 0 little structure, high dispersion

# Third, balanced autocorrelation
dives_sum_filter <- filter(dives_sum, ID == 172062)
acf(dives_sum_filter$mean_dep, na.action = na.pass)       # 1 ACF~8 some variability
acf(dives_sum_filter$max_dep, na.action = na.pass)        # 2 ACF~8 gradual decline
acf(dives_sum_filter$dive_t, na.action = na.pass)         # 2 ACF~7 gradual decline
acf(dives_sum_filter$bott_t, na.action = na.pass)         # 0 ACF~3
acf(dives_sum_filter$prop_bott, na.action = na.pass)      # 0 ACF~2
acf(dives_sum_filter$max_dive_dep, na.action = na.pass)   # 1 gradual but high ACF~22
acf(dives_sum_filter$bott_dist, na.action = na.pass)      # 1 low ACF~4
acf(dives_sum_filter$post_dive_dur, na.action = na.pass)  # 0 low ACF~1

# subjective score
data.frame(var = c("mean_dep", "max_dep", "dive_t", "bott_t", "prop_bott", 
                   "max_dive_dep", "bott_dist", "post_dive_dur"),
           score = c(5, 6, 5, 1, 
                     1, 4, 4, 1))

```
From my subjective interpretation of these outputs, I think the five most promising variables to include are maximum depth, dive time, mean depth, maximum dive depth, and bottom distribution. Next, let's merge the dive data streams the location data.

```{r merge track & dive data.}
tracks_dives <- left_join(prep_tracks_gps_crw_NAgaps, 
                          dives_sum[,c("ID", "time", "max_dep", "dive_t", 
                                       "mean_dep", "max_dive_dep", "bott_dist")], 
                          by = c("ID", "time"))
```

Now, we must check for co-linearity between the data streams to select 1--3 to use in the HMM. We can check co-linearity using the Paerson's correlation matrix.

```{r dive co-linearity}
# calculate and plot check Paerson's correlation matrix
tracks_dives[,c("max_dep", "dive_t", "mean_dep", "max_dive_dep", 
             "bott_dist", "step", "angle")] %>% 
  na.omit() %>%
  cor() %>% 
  corrplot(method="number")
```
`step` and `angle` seem quit independent from all variables as does `bott_dist`. Unfortunately, there is quite high correlation between the first four dive data streams, se we will have to select which to use. `dive_t` and `mean_dep` have the lowest correlation with `bott_dist` and `step`, however `max_dep` had the highest subjective score. For the purposes of this tutorial, I will use `max_dep` and `bott_dist`. 

To get starting parameters, we can fit an HMM to each one independently. We will use the gamma distribution for both and for now will use the same starting parameters.
```{r prep dive data and independent HMMs, message=FALSE}
# identify whether there is 0 data
tracks_dives %>% 
  summarise(max_dep = sum(max_dep == 0, na.rm = T), 
            bott_dist = sum(bott_dist == 0, na.rm = T)) 
# therefore, we need to include zero-mass parameters for bott_dist

# starting parameters (will use same ones for both for now)
mu0 <- c(130, 180)  # mean
sigma0 <- c(60, 90)  # sd
zm <- c(0.1, 0.1)  # zero mass, where applicable

# fit dive-only HMMs
set.seed(1)
HMM_max_dep <- fitHMM(tracks_dives, nbStates = 2, dist = list(max_dep = "gamma"), 
                      Par0 = list(max_dep = c(mu0, sigma0)))
HMM_bott_dist <- fitHMM(tracks_dives, nbStates = 2, dist = list(bott_dist = "gamma"), 
                        Par0 = list(bott_dist = c(mu0, sigma0, zm)))
```
Next, we can integrate these into one HMM together with step length and turning angle. When we specify the starting parameters, we want to think about how the states may look. For example, we might expect one resident state with slower movement, lower angular concentration, deeper dives, and greater at-depth variability. The second state may be travel, with faster movement, greater angular concentration, shallower dives, and less  at-depth variability.

```{r prep data for dive & move HMM}
# prep model
stateNames = c("resident", "travel")
nbStates = length(stateNames)
dist = list(step = "gamma", angle = "vm", max_dep = "gamma", bott_dist =  "gamma")

# Starting Pars 
  # view Pars from previous HMMs
  getPar(HMM_gps_crw_NAgaps)$Par  # state 1 ~ resident, state 2 ~ travel
  getPar(HMM_max_dep)$Par  # state 1 ~ travel, state 2 ~ resident
  getPar(HMM_bott_dist)$Par  # state 1 ~ travel, state 2 ~ resident
  # therefore must switch parameter estimates in HMM_max_dep & HMM_bott_dist
  
  # combine starting Pars 
  step0 <- getPar(HMM_gps_crw_NAgaps)$Par$step
  angle0 <- getPar(HMM_gps_crw_NAgaps)$Par$angle
  max_dep0 <- c(getPar(HMM_max_dep)$Par$max_dep[c(2,1)],  # mu1, mu2
                getPar(HMM_max_dep)$Par$max_dep[c(4,3)])  # sd1, sd2
  bott_dist0 <- c(getPar(HMM_bott_dist)$Par$bott_dist[c(2,1)],  # mu1, mu2
                  getPar(HMM_bott_dist)$Par$bott_dist[c(4,3)],  # sd1, sd2
                  getPar(HMM_bott_dist)$Par$bott_dist[c(6,5)])  # zm1, zm2
  Par0 <- list(step = step0, angle = angle0, max_dep = max_dep0, bott_dist = bott_dist0)
```

Now, we can fit the HMM with movement and dive variables.
```{r fit move & dive HMM, message=F}
set.seed(1)
HMM_move_dive <- fitHMM(tracks_dives, nbStates=nbStates, stateNames=stateNames, dist=dist, Par0=Par0)
```

Let's see what it looks like.
```{r view HMM_move_dive}
plot(HMM_move_dive, breaks = 100, ask = F)
plotPR(HMM_move_dive)
```
The tracks look interesting. But there are some issues in the pseudo residuals.
Looking at the QQ plots the model appears to:
- overestimate the number of fast movement steps
- underestimating higher turning angles
- underestimate shallow dives and over estimate deep dives
- really wonky description of `bott_dist`
The autocorrelation functions suggest there is remnant autocorrelation in `step` and `max_dep` that are not well described by the model. Together, this information suggests that there may be additional states that are not represented. Let's try to add one more state with an intermediate speed, lower angle concentration, and shallow dives.
```{r 3-state dive & move HMM, message=FALSE}
# define states
stateNames <- c("resident", "travel", "search")
nbStates <- length(stateNames)

# Starting Pars 
  # get Pars from last HMM_move_dive HMM
  Pars <- getPar(HMM_move_dive)$Par 

  # combine starting Pars 
  step0 <- c(Pars$step[c(1,2)], mean(Pars$step[c(1,2)]),  # mu
             Pars$step[c(3,4)], mean(Pars$step[c(3,4)]))  # sd
  angle0 <- c(Pars$angle, 3)
  max_dep0 <- c(Pars$max_dep[c(1,2)], 25, # mu
                Pars$max_dep[c(3,4)], 10) # sd
  bott_dist0 <- c(Pars$bott_dist[c(1,2)], mean(Pars$bott_dist[c(1,2)]), # mu
                  Pars$bott_dist[c(3,4)], mean(Pars$bott_dist[c(3,4)]), # sd
                  Pars$bott_dist[c(5,6)], mean(Pars$bott_dist[c(5,6)]))# zm
  Par0 <- list(step = step0, angle = angle0, max_dep = max_dep0, bott_dist = bott_dist0)

# fit 3-state HMM
set.seed(1)
HMM_move_dive_3s <- fitHMM(tracks_dives, nbStates=nbStates, stateNames=stateNames, dist=dist, Par0=Par0)
```

```{r review 3-state HMM}
plotPR(HMM_move_dive_3s)
plot(HMM_move_dive_3s, breaks = 100, ask = F)
```
Interestingly, the `step` and `angle` QQ-plots were not improved much, though the `step` ACF was improved. However, compared to the 2-state model, there was a drastic improvement in QQ-plot and ACF the `max_dep` data stream and marginal improvement in the `bott_dist` data stream. The newly described state appears to have very low step length, really wide turning angle, minimal diving, and minimal at-depth activity, which may actually be indicative of resting. The "resident" state has intermediate speed and turning angle, but very deep dives and high at-depth activity, suggesting it may represent foraging. Let's try to fit one more 4-state HMM, to try and address the remaining residuals: intermediate speed, lower angle concentration, and more intermediate dives, which may represent searching behaviour.
```{r 4-state HMM}
# define states
stateNames <- c("forage", "travel", "rest", "search")
nbStates <- length(stateNames)
# Starting Pars 
  # get Pars from last HMM_move_dive HMM
  Pars <- getPar(HMM_move_dive_3s)$Par 

  # combine starting Pars 
  step0 <- c(Pars$step[c(1:3)], mean(Pars$step[3]),  # mu
             Pars$step[c(4:6)], mean(Pars$step[6]))  # sd
  angle0 <- c(Pars$angle, Pars$angle[3])
  max_dep0 <- c(Pars$max_dep[c(1:2)], Pars$max_dep[3]/2, Pars$max_dep[3], # mu
                Pars$max_dep[c(4:5)], Pars$max_dep[6]/2, Pars$max_dep[6]) # sd
  bott_dist0 <- c(Pars$bott_dist[c(1:3)], Pars$bott_dist[3], # mu
                  Pars$bott_dist[c(4:6)], Pars$bott_dist[6], # sd
                  Pars$bott_dist[c(7:9)], Pars$bott_dist[9]) # zm
  Par0 <- list(step = step0, angle = angle0, max_dep = max_dep0, bott_dist = bott_dist0)

# fit 3-state HM
set.seed(1)
HMM_move_dive_4s <- fitHMM(tracks_dives, nbStates=nbStates, stateNames=stateNames, dist=dist, Par0=Par0)

# view output
plotPR(HMM_move_dive_4s)
plot(HMM_move_dive_4s, breaks = 200)
```
The QQ-plot for `step` is quite a bit improved, as is for `bott_dist`. The ACF for `step` is also quite improved. At this point, I'm not sure the model can be significantly improved with the existing data. Although HMMs with different number of states generally shouldn't be compared, since AIC generally favours more states, it is a good sanity check. 
```{r HMM state AIC}
AIC(HMM_move_dive, HMM_move_dive_3s, HMM_move_dive_4s)
```



## play
```{r}
# plot states
model <- HMM_max_dep
dat <- tracks_dives %>% 
  mutate(max_dep = (max_dep),
         bott_dist = (bott_dist),
         state = viterbi(model)) %>% 
  dplyr::select(max_dep, bott_dist, step, angle, state) 

plot_ly(dat, x = ~log(step), y = ~sqrt(abs(angle)), z = ~-max_dep,color = ~factor(state),  size = 1)
```

One additional option to select which dive data stream to use, is to  and examine which performs better. All of dive data streams are zero-bound, therefore, we can the gamma, log normal, or Weibull distrutions to describe them. For simplicity, we will use the gamma for all, and also divide each data stream by its respective standard deviation to standardise them. 

```{r prep dive data and independent HMMs, message=FALSE}
# standardise dive variables (not necessary)
tracks_dives_prep <- tracks_dives %>% 
  mutate(max_dep = max_dep/sd(max_dep, na.rm = T), 
         dive_t = dive_t/sd(dive_t, na.rm = T), 
         mean_dep = mean_dep/sd(mean_dep, na.rm = T), 
         max_dive_dep = max_dive_dep/sd(max_dive_dep, na.rm = T), 
         bott_dist = bott_dist/sd(bott_dist, na.rm = T)) 

# identify whether there is 0 data
tracks_dives %>% 
  summarise(max_dep = sum(max_dep==0, na.rm = T), 
            dive_t = sum(dive_t==0, na.rm = T), 
            mean_dep = sum(mean_dep==0, na.rm = T), 
            max_dive_dep = sum(max_dive_dep==0, na.rm = T), 
            bott_dist = sum(bott_dist==0, na.rm = T)) 
# therefore, we need to include zero-mass parameters for dive_t and bott_dist

# fit dive-only HMMs
set.seed(1)
HMM_max_dep <- fitHMM(tracks_dives_prep, nbStates=2, dist=list(max_dep="gamma"), 
                      Par0 = list(max_dep = c(mu0, sigma0)))
HMM_dive_t <- fitHMM(tracks_dives_prep, nbStates=2, dist=list(dive_t="gamma"), 
                     Par0 = list(dive_t = c(mu0, sigma0, zm)))
HMM_mean_dep <- fitHMM(tracks_dives_prep, nbStates=2, dist=list(mean_dep="gamma"), 
                       Par0 = list(mean_dep = c(mu0, sigma0)))
HMM_max_dive_dep <- fitHMM(tracks_dives_prep, nbStates=2, dist=list(max_dive_dep="gamma"),
                           Par0 = list(max_dive_dep = c(mu0, sigma0)))
HMM_bott_dist <- fitHMM(tracks_dives_prep, nbStates=2, dist=list(bott_dist="gamma"), 
                        Par0 = list(bott_dist = c(mu0, sigma0, zm)))
```
Now, to compare the models, we cannot use standard metrics of AIC, likelihood, or r$^2$ since the response data (the dive variables) are all different. However, we can look at metrics of 
```{r compare dive-only HMMs, message=FALSE}
plotPR(HMM_max_dep)
plotPR(HMM_dive_t)
plotPR(HMM_mean_dep)
plotPR(HMM_max_dive_dep)
plotPR(HMM_bott_dist)

plot(HMM_max_dep, ask = F)
plot(HMM_dive_t, ask = F)
plot(HMM_mean_dep, ask = T)
plot(HMM_max_dive_dep, ask = T)
plot(HMM_bott_dist, ask = F)
plot(HMM_gps_crw_NAgaps, ask = F)

filter(tracks_dives_prep, ID == "T172062") %>% 
  ggplot(aes(x = x, y = y, col = mean_dep)) + 
  geom_point() + theme_classic()  # 1
filter(tracks_dives_prep, ID == "T172062") %>% 
  ggplot(aes(x = x, y = y, col = max_dep)) + 
  geom_point() + theme_classic()  # 1
filter(tracks_dives_prep, ID == "T172062") %>% 
  ggplot(aes(x = x, y = y, col = dive_t)) + 
  geom_point() + theme_classic() #  1
filter(tracks_dives_prep, ID == "T172062") %>% 
  ggplot(aes(x = x, y = y, col = max_dive_dep)) + 
  geom_point() + theme_classic() # 
filter(tracks_dives_prep, ID == "T172062") %>% 
  ggplot(aes(x = x, y = y, col = bott_dist)) + 
  geom_point() + theme_classic() # 


AIC(HMM_max_dep, HMM_dive_t, HMM_mean_dep, HMM_max_dive_dep, HMM_bott_dist)

f <- which(!is.na(prep_tracks_gps_crw_NAgaps$step))
states_sa <- viterbi(HMM_gps_crw_NAgaps)[f]

data.frame(max_dep = sum(viterbi(HMM_max_dep)[f]==states_sa),
           dive_t = sum(viterbi(HMM_dive_t)[f]==states_sa),
           mean_dep = sum(viterbi(HMM_mean_dep)[f]==states_sa),
           max_dive_dep = sum(viterbi(HMM_max_dive_dep)[f]==states_sa),
           bott_dist = sum(viterbi(HMM_bott_dist)[f]==states_sa))


library(psych)

pairs.panels(tracks_dives_prep[,c("max_dep", "dive_t", "mean_dep", "max_dive_dep", 
             "bott_dist", "step", "angle")],
             gap = 0,
             pch=21)

pairs.panels(cbind(tracks_dives_prep[,c("max_dep", "bott_dist", "step")],
                   abs(tracks_dives_prep[,c("angle")])),
             gap = 0,
             pch=20)
# plot states
dat <- tracks_dives_prep %>% 
  mutate(max_dep = log(max_dep),
         bott_dist = log(bott_dist),
         state = viterbi(HMM_move_dive_3s)) %>% 
  dplyr::select(max_dep, bott_dist, step, angle, state) 
ggplot(dat, aes(x = step, y = angle, col = state)) + 
  geom_point(shape = 20)
ggplot(dat, aes(x = step, y = max_dep, col = state)) + 
  geom_point(shape = 20)
library(plotly)
plot_ly(dat, x = ~log(step), y = ~abs(angle), z = ~-max_dep, color = ~factor(state), size = 1)
plot_ly(dat, x = ~log(step), y = ~sqrt(abs(angle)), z = ~-max_dep,  size = 1)
plot_ly(dat, x = ~log(step), y = ~sqrt(abs(angle)), z = ~-max_dep,color = ~factor(state),  size = 1)
plot_ly(dat, x = ~log(step), y = ~bott_dist, z = ~-max_dep,color = ~factor(state),  size = 1)

plot_ly(dat, x = ~step, y = ~max_dep, z = ~bott_dist, size = 1)
plot_ly(dat, x = ~abs(angle), y = ~max_dep, z = ~bott_dist, size = 1)

# PCA
dat <- tracks_dives_prep %>% 
  mutate(max_dep = log(max_dep),
         bott_dist = log(bott_dist),
         state = viterbi(HMM_move_dive_3s)) %>% 
  dplyr::select(max_dep, bott_dist, step, angle) %>% 
  na.omit() %>% 
  {.[!is.infinite(rowSums(.)),]}

pairs.panels(dat,
             
             gap = 0,
             pch=20)
pc <- prcomp(na.omit(dat[!is.infinite(rowSums(dat[,c("max_dep", "bott_dist", "step", "angle")])),
                         c("max_dep", "bott_dist", "step", "angle")]),
             center = TRUE,
            scale. = TRUE)
print(pc)
ggplot(dat, aes(x = step, y = angle, col = state)) + 
  geom_point()
# PCA
dat <- tracks_dives_prep %>% 
  mutate(mean_dep = log(mean_dep),
         max_dep = log(max_dep),
         dive_t = log(dive_t),
         bott_t = log(bott_t),
         prop_bott = logit(prop_bott),
         max_dive_dep = log(max_dive_dep),
         bott_dist = log(bott_dist),
         post_dive_dur = log(post_dive_dur),
         state = viterbi(HMM_gps_crw_NAgaps)) %>% 
  dplyr::select(max_dep, dive_t, mean_dep, max_dive_dep, bott_dist, step, angle) %>% 
  na.omit() %>% 
  {.[!is.infinite(rowSums(.)),]}

pairs.panels(dat,
             gap = 0,
             pch=21)
dat %>% 
cor() %>% 
  corrplot(method="number")

tracks_dives[,c("max_dep", "dive_t", "mean_dep", "max_dive_dep", 
             "bott_dist", "step", "angle")] %>% 
  na.omit() %>% {.[!is.infinite(rowSums(.)),]} %>% nrow()

pc <- prcomp(na.omit(dat[!is.infinite(rowSums(dat[,c("max_dep", "dive_t", "mean_dep", "max_dive_dep", "bott_dist", "step", "angle")])),
                         c("max_dep", "dive_t", "mean_dep", "max_dive_dep", "bott_dist", "step", "angle")]),
             center = TRUE,
            scale. = TRUE)
attributes(pc)
print(pc)
library(factoextra)
fviz_eig(pc)
groups <- as.factor(na.omit(dat[!is.infinite(rowSums(dat[,c("max_dep", "dive_t", "mean_dep", "max_dive_dep", "bott_dist", "step", "angle")])),])[,"state"])
fviz_pca_ind(pc,
             col.ind = groups, # color by groups
             palette = c("#00AFBB",  "#FC4E07"),
             addEllipses = TRUE, # Concentration ellipses
             ellipse.type = "confidence",
             legend.title = "Groups",
             repel = TRUE
             )

library(ggfortify)
df <- iris[1:4]
pca_res <- prcomp(df, scale. = TRUE)

autoplot(pca_res)
```








```{r calibrate TDR, warning=FALSE}
i = 0
plot(dives$depth*-1, pch = 19, type = 'l', xlim = c(1,8)+8*i, ylim = c(-100,0)); i = i+1
i = 0
plot(dives$dt, pch = 19, xlim = c(1,100)+100*i); i = i+1
# regularise dive
hist(dives$depth,2000, xlim = c(0,200))




# round time to nearest 10 min
# summarise 
## max depth
## mean depth
## 3rd quantile of depth 
## number of dives
## number small vs short dives 
## time at depth
## percent to bottom
## depth variance
## dive type (which dive is most )
# method of summary strongly depends on resolution of HMM. in some cases, individual dives can last longer than the HMM step, and depending on when within the 10 min step a dive starts, we may capture the entirety or just a portion of the dive (e.g., ascent, and descent). Conversely, with lower time locations, we may have a number of dives in each step that must be summarised (e.g., number of dives)

library(terra)
bath <- rast("data/gebco_2022_n74.0_s71.5_w-82.0_e-77.0.tif") %>% 
  crop(ext(-82, -76, 72, 73))





```

```{r plot all tracks}
# convert to lines and plot using tmap
tracks %>% group_by(ID) %>% 
  summarise(do_union = F) %>% 
  st_cast("LINESTRING") %>% 
     tm_shape() +
   tm_lines(col = "ID", palette = "Dark2")

```

```{r}
# first passage time
tracks_high <- filter(tracks, loc_class %in% c("GPS", 3, 2, 1))
ltraj <- as.ltraj(xy = cbind(st_coordinates(tracks_high)[,"X"],
                             st_coordinates(tracks_high)[,"Y"]),
                  date = tracks_high$time, id=factor(tracks_high$ID))
fpt_low <- fpt(ltraj, seq(100, 3000, length=100), units = "hours") %>% 
  varlogfpt() 
fpt_med <- fpt(ltraj, seq(3000, 20000, length=200), units = "hours") %>% 
  varlogfpt()
fpt_hig <- fpt(ltraj, seq(20000, 100000, length=200), units = "hours") %>% 
  varlogfpt()
fpt(ltraj, 1500, units = "hours") %>% unlist %>% mean(na.rm = T)/40
fpt(ltraj, 15000, units = "hours") %>% unlist %>% mean(na.rm = T)/40
fpt(ltraj, 40000, units = "hours") %>% unlist %>% mean(na.rm = T)/40
# looks like there are are increases/plateaus of variance at around 1.5, 15, 40, & 80 km
```

